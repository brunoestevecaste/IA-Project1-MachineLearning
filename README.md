# ü´Ä Predicci√≥n de Enfermedad Card√≠aca - Pipeline Completo

Proyecto de Machine Learning para la predicci√≥n de enfermedades card√≠acas utilizando an√°lisis exploratorio, limpieza de datos avanzada, ingenier√≠a de caracter√≠sticas y modelos de clasificaci√≥n optimizados.

## üìä Estructura del Proyecto

```
‚îú‚îÄ‚îÄ EDA/                          # An√°lisis Exploratorio de Datos
‚îÇ   ‚îú‚îÄ‚îÄ analisis_univariante.py
‚îÇ   ‚îú‚îÄ‚îÄ analisis_multivariante.py
‚îÇ   ‚îî‚îÄ‚îÄ implementation.ipynb
‚îÇ
‚îú‚îÄ‚îÄ Limpieza_IC/                  # Limpieza y Feature Engineering
‚îÇ   ‚îú‚îÄ‚îÄ data_cleaning.py
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_function.py
‚îÇ   ‚îî‚îÄ‚îÄ implementation.ipynb
‚îÇ
‚îî‚îÄ‚îÄ Modelos/                      # Modelos de Clasificaci√≥n
    ‚îú‚îÄ‚îÄ cascade_logistic_model.py
    ‚îú‚îÄ‚îÄ pipeline_function.py
    ‚îî‚îÄ‚îÄ implementation.ipynb
```

## üîç Componentes Principales

### 1. An√°lisis Exploratorio de Datos
- An√°lisis univariante y multivariante de variables
- Visualizaci√≥n de distribuciones y correlaciones
- Identificaci√≥n de patrones en datos faltantes

### 2. Limpieza de Datos e Ingenier√≠a de Caracter√≠sticas
- **Gesti√≥n de valores faltantes y at√≠picos**: Tratamiento diferenciado para valores codificados como `-9` y `?`.
- **Ingenier√≠a de Caracter√≠sticas**: Creaci√≥n de nuevas variables sint√©ticas para capturar patrones de riesgo.
- **Codificaci√≥n**: 
    - *Label encoder* para caracter√≠sticas categ√≥ricas con sentido ordinal.
    - *One-hot encoder* para caracter√≠sticas categ√≥ricas con sentido nominal.
- **Estandarizaci√≥n**: Escalado de caracter√≠sticas mediante Z-Score `StandardScaler`.
- **Modelado y Validaci√≥n**:
    - Comparaci√≥n de estrategias utilizando Regresi√≥n Log√≠stica como modelo base.
    - Validaci√≥n cruzada estratificada `Stratified K-Fold` con 5 particiones.
    - Optimizaci√≥n de hiperpar√°metros mediante Grid Search.
- **Evaluaci√≥n**: Estimaci√≥n del rendimiento y estabilidad del modelo.
$$
\text{Media de la precisi√≥n} \pm \text{Desviaci√≥n est√°ndar}
$$

- **Producci√≥n**: Generaci√≥n autom√°tica del archivo `submission.csv` utilizando la mejor configuraci√≥n encontrada.
- **Visualizaciones**: Gr√°ficas de la distribuci√≥n de los nulos, an√°lisis del bias/varianza y an√°lisis de la importancia de las caracter√≠sticas.
### 3. Modelado
- **Baseline**: Regresi√≥n Log√≠stica con validaci√≥n cruzada estratificada (5-fold)
- **Modelos en cascada**:
  - `CascadedLogisticRegression`: Modelo binario (0 vs >0) + multiclase (1-4)
  - `ThresholdedCascadedLogisticRegression`: Modelo con umbral ajustable para clase 0
- **Optimizaci√≥n**: Grid Search sobre hiperpar√°metros `C`, `penalty`, `solver`

## üìà Resultados Destacados

| Estrategia de Imputaci√≥n `-9` | Estrategia de Imputaci√≥n `?` | CV Accuracy (Test) |
|:------------------------------|:-----------------------------|:------------------:|
| Mediana y Moda                | Mediana y Moda              | **0.59782**        |

**Variable m√°s importante**: `combined_risk` (peso: -0.4175)

## üéØ Conclusiones Clave

1. **Imputaci√≥n simple > M√©todos avanzados**: La imputaci√≥n por mediana/moda demostr√≥ mejor generalizaci√≥n que KNN y MICE, evitando sobreajuste en datasets peque√±os.

2. **Feature Engineering cr√≠tico**: Las variables sint√©ticas (`combined_risk`, `age_chol_interaction`) superaron en importancia a muchas variables originales.

3. **Alto bias, baja varianza**: El modelo sufre de subajuste m√°s que de sobreajuste. La mejora no vendr√° de modelos m√°s complejos, sino de mejores features o recuperar variables eliminadas.

4. **Regularizaci√≥n moderada**: Una regularizaci√≥n fuerte (`C` peque√±a) fue clave para controlar el ruido inherente en el dataset.

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Python 3.x**
- **Scikit-learn**: Modelado, validaci√≥n cruzada, Grid Search
- **Pandas/NumPy**: Manipulaci√≥n de datos
- **Plotly**: Visualizaciones

## üöÄ Uso

Cada carpeta contiene un notebook `implementation.ipynb` con el flujo completo:

```bash
# Ejemplo: Ejecutar pipeline de limpieza
cd Limpieza_IC
jupyter notebook implementation.ipynb
```

## üìù Notas Adicionales

- El dataset presenta desbalanceo de clases y alta tasa de valores faltantes en variables clave.
- La eliminaci√≥n de `ca` y `thal` redujo el poder predictivo pero mejor√≥ la estabilidad.
- Los modelos en cascada se exploraron para mejorar la discriminaci√≥n de la clase 0.

---

**Autores**: Marta Soler Ebri, Javier Gracia, Bruno Esteve, Ignacio Benlloch 
**Fecha**: Diciembre 2025
